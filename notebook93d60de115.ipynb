{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10943620,"sourceType":"datasetVersion","datasetId":6806271}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets peft accelerate bitsandbytes evaluate rouge-score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:49:33.599841Z","iopub.execute_input":"2025-03-06T19:49:33.600134Z","iopub.status.idle":"2025-03-06T19:49:37.018881Z","shell.execute_reply.started":"2025-03-06T19:49:33.600113Z","shell.execute_reply":"2025-03-06T19:49:37.017909Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Assuming the data is uploaded to following PATH in kaggle\nFILEPATH = \"/kaggle/input/query-data/data.csv\"\n\n# Create dataset\nfrom datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=FILEPATH, encoding=\"latin1\")\ndataset = dataset[\"train\"].train_test_split(test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:43:22.541400Z","iopub.execute_input":"2025-03-06T19:43:22.541725Z","iopub.status.idle":"2025-03-06T19:43:24.020644Z","shell.execute_reply.started":"2025-03-06T19:43:22.541680Z","shell.execute_reply":"2025-03-06T19:43:24.019867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c0d4a3564fb45849f11bb9fde35c1f1"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments, Trainer\n\nmodel_name = \"t5-small\"  # or distilT5\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    model_inputs = tokenizer(examples[\"original_query\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    # Tokenizing the optimized query for the decoder side\n    labels = tokenizer(examples[\"optimized_query\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    # For decoder input, the decoder_input_ids are typically the same as labels\n    model_inputs[\"decoder_input_ids\"] = labels[\"input_ids\"]  # Decoder input ids\n    \n    # Labels are used as the targets (to compute loss)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n\ntokenized_dataset = dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T20:00:31.231607Z","iopub.execute_input":"2025-03-06T20:00:31.231895Z","iopub.status.idle":"2025-03-06T20:00:35.684159Z","shell.execute_reply.started":"2025-03-06T20:00:31.231876Z","shell.execute_reply":"2025-03-06T20:00:35.682973Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edeae6343aa5433d89f01b5014bed8b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9867e6965a4ed3ac5856077f7cae04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99d14b9c9e54cb0af8bb5886caa35b1"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ace1ed505604c03bfe7d17cbda53aea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"896b7d65c3e9483c88aa4e0a350c4c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e99add7afdb24a7da0824fc01b66ec6e"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-04620aeaf3f6>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtokenized_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"],"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error"}],"execution_count":4},{"cell_type":"code","source":"# LORA configuration\nlora_config = LoraConfig(\n    r=8,                   # Rank of the low-rank adapters\n    lora_alpha=16,         # Scaling factor for low-rank adaptation\n    target_modules=[\"q\", \"k\", \"v\"],  # Common targets for transformers\n    lora_dropout=0.05,     # Dropout for regularization\n    bias=\"none\",           # No bias in the low-rank adapters\n    task_type=\"SEQ_2_SEQ_LM\"\n)\n\n# Apply LORA to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-06T19:41:48.765Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\n# Load the metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_metrics(p):\n   \n    # Extract logits and labels from the tuple\n    predictions, labels = p.predictions, p.label_ids\n\n    print(f\"Predictions type: {type(predictions)}\")\n    print(f\"Labels type: {type(labels)}\")\n    \n    # Check if predictions are logits (3D shape: [batch_size, seq_length, vocab_size])\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]  # If predictions is a tuple, take the logits (first element)\n    \n    if predictions.ndim == 3:\n        # Apply argmax to get the token IDs (from logits to token IDs)\n        predictions = np.argmax(predictions, axis=-1)  # Get the token IDs\n\n    # Flatten the predictions and labels if they are nested lists\n    if isinstance(predictions[0], list):\n        predictions = [item for sublist in predictions for item in sublist]  # Flatten predictions\n    if isinstance(labels[0], list):\n        labels = [item for sublist in labels for item in sublist]  # Flatten labels\n\n    # Decode predictions and labels using tokenizer\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n\n    # Compute ROUGE score\n    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n\n    return {\n        \"bleu\": bleu_score[\"bleu\"],\n        \"rouge1\": rouge_score[\"rouge1\"],\n        \"rouge2\": rouge_score[\"rouge2\"],\n        \"rougeL\": rouge_score[\"rougeL\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:44:12.479251Z","iopub.execute_input":"2025-03-06T19:44:12.479569Z","iopub.status.idle":"2025-03-06T19:44:15.602232Z","shell.execute_reply.started":"2025-03-06T19:44:12.479543Z","shell.execute_reply":"2025-03-06T19:44:15.601570Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e48f1e1ae9480c93f3a6bf075bb331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5239a832cbf458aac28f7d74214501e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8688ba9a05f649b8aa057e8f99bba33f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79258088bbeb4795bad3c5146d2fa6d0"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./output\",\n    logging_dir=\"./logs\",  # Optional: This will store logs for future use with TensorBoard\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    learning_rate=2e-4,\n    fp16=True,\n    evaluation_strategy=\"no\",\n    save_steps=500,\n    logging_steps=100,\n    save_total_limit=2,\n    report_to=\"none\",  # Disable WandB\n    logging_first_step=True,  # Log at the first step to ensure visibility\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:44:17.929963Z","iopub.execute_input":"2025-03-06T19:44:17.930285Z","iopub.status.idle":"2025-03-06T19:44:18.098143Z","shell.execute_reply.started":"2025-03-06T19:44:17.930258Z","shell.execute_reply":"2025-03-06T19:44:18.097107Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    args=training_args,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:44:20.835747Z","iopub.execute_input":"2025-03-06T19:44:20.836097Z","iopub.status.idle":"2025-03-06T19:44:21.203954Z","shell.execute_reply.started":"2025-03-06T19:44:20.836071Z","shell.execute_reply":"2025-03-06T19:44:21.203249Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T18:54:27.883984Z","iopub.execute_input":"2025-03-06T18:54:27.884288Z","iopub.status.idle":"2025-03-06T19:22:23.007633Z","shell.execute_reply.started":"2025-03-06T18:54:27.884267Z","shell.execute_reply":"2025-03-06T19:22:23.006782Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1200/1200 27:53, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.018000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.019800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.014500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.011300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.011000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.010400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.009800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.009400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.008800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.008800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1200, training_loss=0.011854723157982031, metrics={'train_runtime': 1674.4745, 'train_samples_per_second': 22.933, 'train_steps_per_second': 0.717, 'total_flos': 5249309029171200.0, 'train_loss': 0.011854723157982031, 'epoch': 3.0})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model.save_pretrained(\"./output_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:39:47.804194Z","iopub.execute_input":"2025-03-06T19:39:47.804527Z","iopub.status.idle":"2025-03-06T19:39:48.003985Z","shell.execute_reply.started":"2025-03-06T19:39:47.804499Z","shell.execute_reply":"2025-03-06T19:39:48.003229Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"!pip install peft --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:53:49.881712Z","iopub.execute_input":"2025-03-06T19:53:49.882011Z","iopub.status.idle":"2025-03-06T19:53:55.237304Z","shell.execute_reply.started":"2025-03-06T19:53:49.881990Z","shell.execute_reply":"2025-03-06T19:53:55.236214Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.47.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.2.1)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->peft) (2.4.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.21.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->peft) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->peft) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->peft) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->peft) (2024.2.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import sys\nsys.modules.clear()  # Clear all imports","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-06T19:58:26.801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/output_model\", 'zip', \"/kaggle/working/output_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:59:01.365027Z","iopub.execute_input":"2025-03-06T19:59:01.365254Z","iopub.status.idle":"2025-03-06T19:59:01.422773Z","shell.execute_reply.started":"2025-03-06T19:59:01.365235Z","shell.execute_reply":"2025-03-06T19:59:01.422035Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/output_model.zip'"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import peft\nprint(peft.__version__)\n\nfrom peft import get_peft_model, T5AdapterModel\n\n# Check the contents of the peft module\nprint(dir(peft))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-06T19:58:26.801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.args.per_device_eval_batch_size = 16\ntrainer.args.device=device\nmetrics = trainer.evaluate()\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T19:34:08.677681Z","iopub.execute_input":"2025-03-06T19:34:08.677994Z","iopub.status.idle":"2025-03-06T19:34:08.702885Z","shell.execute_reply.started":"2025-03-06T19:34:08.677971Z","shell.execute_reply":"2025-03-06T19:34:08.701775Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-daf8c661a34f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mper_device_eval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: can't set attribute 'device'"],"ename":"AttributeError","evalue":"can't set attribute 'device'","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import get_peft_model  # LoRA-specific function\n\n# Load the model and tokenizer\nmodel_name = \"t5-small\"  # Or the specific model name you fine-tuned\nmodel = T5ForConditionalGeneration.from_pretrained('./output/checkpoint-1200')  # Load from the fine-tuned checkpoint\ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Apply LoRA (Adapter) to the model\nmodel = get_peft_model(model, peft_config=lora_config)  # This integrates the LoRA adapters into the base model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T17:42:18.181794Z","iopub.execute_input":"2025-03-06T17:42:18.182155Z","iopub.status.idle":"2025-03-06T17:42:18.865853Z","shell.execute_reply.started":"2025-03-06T17:42:18.182126Z","shell.execute_reply":"2025-03-06T17:42:18.864968Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T17:42:08.555191Z","iopub.execute_input":"2025-03-06T17:42:08.555491Z","iopub.status.idle":"2025-03-06T17:42:08.558805Z","shell.execute_reply.started":"2025-03-06T17:42:08.555468Z","shell.execute_reply":"2025-03-06T17:42:08.557959Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import torch\n# Example input query\ninput_text = \"Optimize the query: 'average earnings of college graduate'\"\n\n# Tokenize the input\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\n# Perform inference with the model (access the base model's generate method)\nwith torch.no_grad():\n    # Using model.base_model to access the original generate method\n    outputs = model.generate(input_ids=input_ids, max_length=50, num_beams=5, early_stopping=True)\n\n# Decode the generated output\noutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Output:\", output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T17:50:32.236177Z","iopub.execute_input":"2025-03-06T17:50:32.236486Z","iopub.status.idle":"2025-03-06T17:50:32.446705Z","shell.execute_reply.started":"2025-03-06T17:50:32.236462Z","shell.execute_reply":"2025-03-06T17:50:32.445932Z"}},"outputs":[{"name":"stdout","text":"Generated Output: Optimize the query\n","output_type":"stream"}],"execution_count":29}]}