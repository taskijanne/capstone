{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10982851,"sourceType":"datasetVersion","datasetId":6835192},{"sourceId":10982904,"sourceType":"datasetVersion","datasetId":6835233},{"sourceId":10982918,"sourceType":"datasetVersion","datasetId":6835246},{"sourceId":10982945,"sourceType":"datasetVersion","datasetId":6835267},{"sourceId":10983062,"sourceType":"datasetVersion","datasetId":6835362}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This is the notebook that was used to fine-tune t5-small model with \"combined_queries.csv\" -dataset\nThe dataset included 60000 examples of \"Bad queries\" and \"Optimized queries\"","metadata":{}},{"cell_type":"code","source":"# Assuming the data is uploaded to following PATH in kaggle\nFILEPATH = \"/kaggle/input/query-data-11/combined_queries.csv\"\n\n# Create dataset\nfrom datasets import load_dataset\ndataset = load_dataset(\"csv\", data_files=FILEPATH, encoding=\"latin1\")\ndataset = dataset[\"train\"].train_test_split(test_size=0.2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:59:41.384446Z","iopub.execute_input":"2025-03-10T17:59:41.384805Z","iopub.status.idle":"2025-03-10T17:59:44.247739Z","shell.execute_reply.started":"2025-03-10T17:59:41.384774Z","shell.execute_reply":"2025-03-10T17:59:44.247086Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9f4eaeb641a474891ca1652c4f6eae7"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"!pip install -q transformers datasets peft accelerate bitsandbytes evaluate rouge-score","metadata":{"execution":{"iopub.status.busy":"2025-03-10T17:58:45.521396Z","iopub.execute_input":"2025-03-10T17:58:45.521693Z","iopub.status.idle":"2025-03-10T17:58:57.435682Z","shell.execute_reply.started":"2025-03-10T17:58:45.521665Z","shell.execute_reply":"2025-03-10T17:58:57.434641Z"}}},{"cell_type":"code","source":"print(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:14:43.285945Z","iopub.execute_input":"2025-03-10T18:14:43.286246Z","iopub.status.idle":"2025-03-10T18:14:43.290699Z","shell.execute_reply.started":"2025-03-10T18:14:43.286223Z","shell.execute_reply":"2025-03-10T18:14:43.289792Z"}},"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Original Query', 'Optimized Query'],\n        num_rows: 48000\n    })\n    test: Dataset({\n        features: ['Original Query', 'Optimized Query'],\n        num_rows: 12000\n    })\n})\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import LoraConfig, get_peft_model\nfrom transformers import TrainingArguments, Trainer\n\nmodel_name = \"t5-small\"  # or distilT5\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\ndef tokenize_function(examples):\n    # Tokenize the original query (input to the model)\n    model_inputs = tokenizer(examples[\"Original Query\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    # Tokenize the optimized query (target/output for the model)\n    labels = tokenizer(examples[\"Optimized Query\"], padding=\"max_length\", truncation=True, max_length=512)\n    \n    # Shift labels for decoder input (decoder input is typically shifted by one token)\n    decoder_input_ids = [tokenizer.pad_token_id] + labels[\"input_ids\"][:-1]  # Add padding token at the start and shift\n    \n    # Make sure that the returned values are lists (not just single tokens or strings)\n    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    return model_inputs\n\n\ntokenized_dataset = dataset.map(tokenize_function)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:14:50.360498Z","iopub.execute_input":"2025-03-10T18:14:50.360827Z","iopub.status.idle":"2025-03-10T18:15:43.397818Z","shell.execute_reply.started":"2025-03-10T18:14:50.360800Z","shell.execute_reply":"2025-03-10T18:15:43.396891Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/48000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f5ba2930e604f2b875094e2ca36a27e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03057186d4a243f7a674a81fea3ab7ff"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# LORA configuration\n\"\"\"lora_config = LoraConfig(\n    r=8,                   # Rank of the low-rank adapters\n    lora_alpha=16,         # Scaling factor for low-rank adaptation\n    target_modules=[\"q\", \"k\", \"v\"],  # Common targets for transformers\n    lora_dropout=0.05,     # Dropout for regularization\n    bias=\"none\",           # No bias in the low-rank adapters\n    task_type=\"SEQ_2_SEQ_LM\"\n)\"\"\"\n\nlora_config = LoraConfig(\n    r=8,                    # Rank of the low-rank adapters\n    lora_alpha=16,          # Scaling factor for low-rank adaptation\n    target_modules=[\n        \"q\", \"k\", \"v\",                    # Self-attention matrices\n        \"EncDecAttention.q\", \"EncDecAttention.k\", \"EncDecAttention.v\",  # Cross-attention matrices in the decoder\n        \"wi\", \"wo\"                        # Feedforward layers' weight matrices (optional)\n    ],\n    lora_dropout=0.05,      # Dropout for regularization\n    bias=\"none\",            # No bias in the low-rank adapters\n    task_type=\"SEQ_2_SEQ_LM\"\n)\n\n# Apply LORA to the model\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:19:43.181475Z","iopub.execute_input":"2025-03-10T18:19:43.181817Z","iopub.status.idle":"2025-03-10T18:19:44.041434Z","shell.execute_reply.started":"2025-03-10T18:19:43.181789Z","shell.execute_reply":"2025-03-10T18:19:44.040670Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"import numpy as np\nimport evaluate\n\n# Load the metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n\ndef compute_metrics(p):\n   \n    # Extract logits and labels from the tuple\n    predictions, labels = p.predictions, p.label_ids\n\n    print(f\"Predictions type: {type(predictions)}\")\n    print(f\"Labels type: {type(labels)}\")\n    \n    # Check if predictions are logits (3D shape: [batch_size, seq_length, vocab_size])\n    if isinstance(predictions, tuple):\n        predictions = predictions[0]  # If predictions is a tuple, take the logits (first element)\n    \n    if predictions.ndim == 3:\n        # Apply argmax to get the token IDs (from logits to token IDs)\n        predictions = np.argmax(predictions, axis=-1)  # Get the token IDs\n\n    # Flatten the predictions and labels if they are nested lists\n    if isinstance(predictions[0], list):\n        predictions = [item for sublist in predictions for item in sublist]  # Flatten predictions\n    if isinstance(labels[0], list):\n        labels = [item for sublist in labels for item in sublist]  # Flatten labels\n\n    # Decode predictions and labels using tokenizer\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Compute BLEU score\n    bleu_score = bleu_metric.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])\n\n    # Compute ROUGE score\n    rouge_score = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n\n    return {\n        \"bleu\": bleu_score[\"bleu\"],\n        \"rouge1\": rouge_score[\"rouge1\"],\n        \"rouge2\": rouge_score[\"rouge2\"],\n        \"rougeL\": rouge_score[\"rougeL\"],\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:19:46.282435Z","iopub.execute_input":"2025-03-10T18:19:46.282851Z","iopub.status.idle":"2025-03-10T18:19:48.970410Z","shell.execute_reply.started":"2025-03-10T18:19:46.282817Z","shell.execute_reply":"2025-03-10T18:19:48.969783Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d703672fe7a3470bb96c69a570f9d693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36bb9cbcc1b2414caf171ec09803ead6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de3efd9869e476c88d65c49b79ae0ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4ac34c52a941aa8c7fccf6c02c2bdd"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"import torch\n\nif torch.cuda.device_count() > 1:\n    model = torch.nn.DataParallel(model)\n\nmodel.to(\"cuda\")  # Move the model to GPU","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:19:50.683399Z","iopub.execute_input":"2025-03-10T18:19:50.683726Z","iopub.status.idle":"2025-03-10T18:19:51.078550Z","shell.execute_reply.started":"2025-03-10T18:19:50.683700Z","shell.execute_reply":"2025-03-10T18:19:51.077632Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"DataParallel(\n  (module): PeftModelForSeq2SeqLM(\n    (base_model): LoraModel(\n      (model): T5ForConditionalGeneration(\n        (shared): Embedding(32128, 512)\n        (encoder): T5Stack(\n          (embed_tokens): Embedding(32128, 512)\n          (block): ModuleList(\n            (0): T5Block(\n              (layer): ModuleList(\n                (0): T5LayerSelfAttention(\n                  (SelfAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                    (relative_attention_bias): Embedding(32, 8)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (1): T5LayerFF(\n                  (DenseReluDense): T5DenseActDense(\n                    (wi): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=2048, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=2048, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (wo): lora.Linear(\n                      (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=2048, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.1, inplace=False)\n                    (act): ReLU()\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n            (1-5): 5 x T5Block(\n              (layer): ModuleList(\n                (0): T5LayerSelfAttention(\n                  (SelfAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (1): T5LayerFF(\n                  (DenseReluDense): T5DenseActDense(\n                    (wi): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=2048, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=2048, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (wo): lora.Linear(\n                      (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=2048, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.1, inplace=False)\n                    (act): ReLU()\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n          (final_layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (decoder): T5Stack(\n          (embed_tokens): Embedding(32128, 512)\n          (block): ModuleList(\n            (0): T5Block(\n              (layer): ModuleList(\n                (0): T5LayerSelfAttention(\n                  (SelfAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                    (relative_attention_bias): Embedding(32, 8)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (1): T5LayerCrossAttention(\n                  (EncDecAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (2): T5LayerFF(\n                  (DenseReluDense): T5DenseActDense(\n                    (wi): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=2048, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=2048, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (wo): lora.Linear(\n                      (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=2048, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.1, inplace=False)\n                    (act): ReLU()\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n            (1-5): 5 x T5Block(\n              (layer): ModuleList(\n                (0): T5LayerSelfAttention(\n                  (SelfAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (1): T5LayerCrossAttention(\n                  (EncDecAttention): T5Attention(\n                    (q): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (k): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (v): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (o): Linear(in_features=512, out_features=512, bias=False)\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (2): T5LayerFF(\n                  (DenseReluDense): T5DenseActDense(\n                    (wi): lora.Linear(\n                      (base_layer): Linear(in_features=512, out_features=2048, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=512, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=2048, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (wo): lora.Linear(\n                      (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n                      (lora_dropout): ModuleDict(\n                        (default): Dropout(p=0.05, inplace=False)\n                      )\n                      (lora_A): ModuleDict(\n                        (default): Linear(in_features=2048, out_features=8, bias=False)\n                      )\n                      (lora_B): ModuleDict(\n                        (default): Linear(in_features=8, out_features=512, bias=False)\n                      )\n                      (lora_embedding_A): ParameterDict()\n                      (lora_embedding_B): ParameterDict()\n                      (lora_magnitude_vector): ModuleDict()\n                    )\n                    (dropout): Dropout(p=0.1, inplace=False)\n                    (act): ReLU()\n                  )\n                  (layer_norm): T5LayerNorm()\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n            )\n          )\n          (final_layer_norm): T5LayerNorm()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./output\",\n    logging_dir=\"./logs\",  # Optional: This will store logs for future use with TensorBoard\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    learning_rate=2e-4,\n    fp16=True,\n    evaluation_strategy=\"no\",\n    save_steps=500,\n    logging_steps=100,\n    save_total_limit=2,\n    report_to=\"none\",  # Disable WandB\n    logging_first_step=True,  # Log at the first step to ensure visibility\n    ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,\n    remove_unused_columns=False\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:19:55.582755Z","iopub.execute_input":"2025-03-10T18:19:55.583061Z","iopub.status.idle":"2025-03-10T18:19:55.623116Z","shell.execute_reply.started":"2025-03-10T18:19:55.583035Z","shell.execute_reply":"2025-03-10T18:19:55.622283Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    args=training_args,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:19:57.832153Z","iopub.execute_input":"2025-03-10T18:19:57.832473Z","iopub.status.idle":"2025-03-10T18:19:57.871798Z","shell.execute_reply.started":"2025-03-10T18:19:57.832445Z","shell.execute_reply":"2025-03-10T18:19:57.870950Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T18:20:04.890527Z","iopub.execute_input":"2025-03-10T18:20:04.890866Z","iopub.status.idle":"2025-03-10T20:17:42.417288Z","shell.execute_reply.started":"2025-03-10T18:20:04.890838Z","shell.execute_reply":"2025-03-10T20:17:42.416494Z"}},"outputs":[{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7500/7500 1:57:32, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>12.598900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.578700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.127500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.082900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.062500</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.044500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.031700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.026900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.023500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.021700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.020300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.020000</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.018600</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.018300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.017800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>0.016800</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.016200</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.016100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.015800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.015300</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.015200</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.013900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.014600</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.014200</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.014100</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.013700</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.014000</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.014400</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.013900</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.013300</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.012700</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.013800</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.012900</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.012800</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.013100</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.012800</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.013000</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.013200</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.012600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7500, training_loss=0.08061803852717082, metrics={'train_runtime': 7057.1573, 'train_samples_per_second': 34.008, 'train_steps_per_second': 1.063, 'total_flos': 0.0, 'train_loss': 0.08061803852717082, 'epoch': 5.0})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"trainer.model.module.save_pretrained(\"./output_model_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:27:19.079757Z","iopub.execute_input":"2025-03-10T17:27:19.080084Z","iopub.status.idle":"2025-03-10T17:27:19.206238Z","shell.execute_reply.started":"2025-03-10T17:27:19.080061Z","shell.execute_reply":"2025-03-10T17:27:19.205512Z"}},"outputs":[],"execution_count":79},{"cell_type":"code","source":"from peft import PeftModel\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel_name = \"t5-small\"  # or distilT5\nbase_model = T5ForConditionalGeneration.from_pretrained(model_name)  # Load base model\nmodell = PeftModel.from_pretrained(base_model,'./output_model_3', lora_config=lora_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T20:19:43.431895Z","iopub.execute_input":"2025-03-10T20:19:43.432221Z","iopub.status.idle":"2025-03-10T20:19:44.122774Z","shell.execute_reply.started":"2025-03-10T20:19:43.432179Z","shell.execute_reply":"2025-03-10T20:19:44.121853Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"model.module.save_pretrained(\"./output_model_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T20:18:46.104603Z","iopub.execute_input":"2025-03-10T20:18:46.104940Z","iopub.status.idle":"2025-03-10T20:18:46.269020Z","shell.execute_reply.started":"2025-03-10T20:18:46.104911Z","shell.execute_reply":"2025-03-10T20:18:46.268348Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import sys\nsys.modules.clear()  # Clear all imports","metadata":{"trusted":true,"execution":{"execution_failed":"2025-03-06T19:58:26.801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/output_model_3\", 'zip', \"/kaggle/working/output_model_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T20:19:04.954526Z","iopub.execute_input":"2025-03-10T20:19:04.954865Z","iopub.status.idle":"2025-03-10T20:19:05.144660Z","shell.execute_reply.started":"2025-03-10T20:19:04.954840Z","shell.execute_reply":"2025-03-10T20:19:05.143957Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/output_model_3.zip'"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"modell = modell.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:35:50.826302Z","iopub.execute_input":"2025-03-10T17:35:50.826620Z","iopub.status.idle":"2025-03-10T17:35:50.976303Z","shell.execute_reply.started":"2025-03-10T17:35:50.826594Z","shell.execute_reply":"2025-03-10T17:35:50.975283Z"}},"outputs":[],"execution_count":108},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\n\n# Load the T5 model\nmodel_name = \"t5-small\"\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:43:25.679098Z","iopub.execute_input":"2025-03-10T17:43:25.679450Z","iopub.status.idle":"2025-03-10T17:43:26.028146Z","shell.execute_reply.started":"2025-03-10T17:43:25.679425Z","shell.execute_reply":"2025-03-10T17:43:26.027153Z"}},"outputs":[{"name":"stdout","text":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 8)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-5): 5 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=512, bias=False)\n              (k): Linear(in_features=512, out_features=512, bias=False)\n              (v): Linear(in_features=512, out_features=512, bias=False)\n              (o): Linear(in_features=512, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseActDense(\n              (wi): Linear(in_features=512, out_features=2048, bias=False)\n              (wo): Linear(in_features=2048, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): ReLU()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)\n","output_type":"stream"}],"execution_count":117},{"cell_type":"code","source":"import torch\n# Example input query\ninput_text = \"optimize how much does golden retriever weight in kilograms\"\n\n# Tokenize the input\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n\n# Perform inference with the model (access the base model's generate method)\nwith torch.no_grad():\n    # Using model.base_model to access the original generate method\n    outputs = modell.generate(input_ids=input_ids, max_length=128)\n\n# Decode the generated output\noutput_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(\"Generated Output:\", output_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T20:24:31.180369Z","iopub.execute_input":"2025-03-10T20:24:31.180711Z","iopub.status.idle":"2025-03-10T20:24:31.384074Z","shell.execute_reply.started":"2025-03-10T20:24:31.180682Z","shell.execute_reply":"2025-03-10T20:24:31.383098Z"}},"outputs":[{"name":"stdout","text":"Generated Output: golden retriever weight in kilograms\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"print(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T17:36:23.904898Z","iopub.execute_input":"2025-03-10T17:36:23.905328Z","iopub.status.idle":"2025-03-10T17:36:23.910979Z","shell.execute_reply.started":"2025-03-10T17:36:23.905295Z","shell.execute_reply":"2025-03-10T17:36:23.909987Z"}},"outputs":[{"name":"stdout","text":"tensor([[    0, 13436,   125,    19,    48,     1]])\n","output_type":"stream"}],"execution_count":114}]}